#!/usr/bin/env python3
"""
Practical System Load Test for TraceTrack
Tests system with 50+ concurrent users and high volume of bags
"""

import asyncio
import aiohttp
import time
import random
import string
import logging
import sys
import json
from datetime import datetime
import psutil
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'load_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Test configuration
BASE_URL = "http://0.0.0.0:5000"
CONCURRENT_USERS = 55  # 50+ concurrent users
TEST_DURATION_SECONDS = 60  # 1 minute initial test
REQUEST_TIMEOUT = 30

class SystemMetrics:
    """Track system performance metrics"""
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.error_count = 0
        self.response_times = []
        self.endpoint_stats = {}
        self.lock = threading.Lock()
        
    def record_request(self, endpoint, duration, status_code, success):
        """Thread-safe request recording"""
        with self.lock:
            self.request_count += 1
            self.response_times.append(duration)
            
            if not success:
                self.error_count += 1
                
            if endpoint not in self.endpoint_stats:
                self.endpoint_stats[endpoint] = {
                    'count': 0,
                    'errors': 0,
                    'total_time': 0,
                    'status_codes': {}
                }
            
            stats = self.endpoint_stats[endpoint]
            stats['count'] += 1
            stats['total_time'] += duration
            if not success:
                stats['errors'] += 1
            
            status_code_str = str(status_code)
            if status_code_str not in stats['status_codes']:
                stats['status_codes'][status_code_str] = 0
            stats['status_codes'][status_code_str] += 1
    
    def get_summary(self):
        """Get performance summary"""
        if not self.response_times:
            return {"error": "No requests recorded"}
        
        sorted_times = sorted(self.response_times)
        total_reqs = len(self.response_times)
        duration = time.time() - self.start_time
        
        return {
            "test_duration": duration,
            "total_requests": total_reqs,
            "successful_requests": total_reqs - self.error_count,
            "failed_requests": self.error_count,
            "error_rate": (self.error_count / total_reqs * 100) if total_reqs > 0 else 0,
            "requests_per_second": total_reqs / duration if duration > 0 else 0,
            "avg_response_time": sum(self.response_times) / total_reqs,
            "min_response_time": min(self.response_times),
            "max_response_time": max(self.response_times),
            "p50_response_time": sorted_times[int(total_reqs * 0.5)] if total_reqs > 0 else 0,
            "p95_response_time": sorted_times[int(total_reqs * 0.95)] if total_reqs > 0 else 0,
            "p99_response_time": sorted_times[int(total_reqs * 0.99)] if total_reqs > 0 else 0,
            "endpoint_stats": self.endpoint_stats
        }

class LoadTester:
    """Main load testing class"""
    
    def __init__(self):
        self.metrics = SystemMetrics()
        self.test_users = []
        self.test_data = {
            'parent_bags': [],
            'child_bags': [],
            'bills': []
        }
        
    def generate_qr_code(self, prefix="QR"):
        """Generate a unique QR code"""
        timestamp = int(time.time() * 1000)
        random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))
        return f"{prefix}_{timestamp}_{random_str}"
    
    def create_test_session(self):
        """Create a new test session"""
        return aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT),
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=30)
        )
    
    async def test_endpoint(self, session, method, url, data=None, json_data=None):
        """Test a single endpoint"""
        start_time = time.time()
        try:
            if method == "GET":
                async with session.get(url) as response:
                    duration = time.time() - start_time
                    success = response.status in [200, 304]
                    endpoint = url.replace(BASE_URL, "")
                    self.metrics.record_request(endpoint, duration, response.status, success)
                    return response.status, await response.text()
                    
            elif method == "POST":
                kwargs = {}
                if json_data:
                    kwargs['json'] = json_data
                elif data:
                    kwargs['data'] = data
                    
                async with session.post(url, **kwargs) as response:
                    duration = time.time() - start_time
                    success = response.status in [200, 201, 302, 303]
                    endpoint = url.replace(BASE_URL, "")
                    self.metrics.record_request(endpoint, duration, response.status, success)
                    return response.status, await response.text()
                    
        except asyncio.TimeoutError:
            duration = time.time() - start_time
            endpoint = url.replace(BASE_URL, "")
            self.metrics.record_request(endpoint, duration, 0, False)
            logger.error(f"Timeout on {endpoint}")
            return 0, "Timeout"
        except Exception as e:
            duration = time.time() - start_time
            endpoint = url.replace(BASE_URL, "")
            self.metrics.record_request(endpoint, duration, 0, False)
            logger.error(f"Error on {endpoint}: {str(e)}")
            return 0, str(e)
    
    async def login_user(self, session, username="admin", password="admin123"):
        """Login a user and maintain session"""
        status, response = await self.test_endpoint(
            session, "POST", f"{BASE_URL}/login",
            data={"username": username, "password": password}
        )
        return status in [200, 302, 303]
    
    async def simulate_user_workflow(self, user_id):
        """Simulate a single user's workflow"""
        session = self.create_test_session()
        
        try:
            # Login as admin (or create test user if needed)
            logged_in = await self.login_user(session)
            if not logged_in:
                logger.warning(f"User {user_id} failed to login")
                return
            
            # Simulate different user behaviors
            behavior = random.choice(['scanner', 'viewer', 'biller', 'admin'])
            
            if behavior == 'scanner':
                # Simulate QR code scanning workflow
                for _ in range(random.randint(5, 10)):
                    # Scan parent bag
                    parent_qr = self.generate_qr_code("PARENT")
                    await self.test_endpoint(
                        session, "POST", f"{BASE_URL}/scan/parent",
                        json_data={"qr_code": parent_qr}
                    )
                    
                    # Scan child bags
                    child_qrs = [self.generate_qr_code("CHILD") for _ in range(random.randint(5, 15))]
                    await self.test_endpoint(
                        session, "POST", f"{BASE_URL}/process_child_scan_fast",
                        json_data={"parent_qr": parent_qr, "child_qrs": child_qrs}
                    )
                    
                    # Small delay between scans
                    await asyncio.sleep(random.uniform(0.5, 2))
                    
            elif behavior == 'viewer':
                # Simulate viewing pages
                pages = ['/dashboard', '/bags', '/bills', '/scans', '/api/stats']
                for _ in range(random.randint(10, 20)):
                    page = random.choice(pages)
                    await self.test_endpoint(session, "GET", f"{BASE_URL}{page}")
                    await asyncio.sleep(random.uniform(0.5, 1.5))
                    
            elif behavior == 'biller':
                # Simulate bill operations
                for _ in range(random.randint(3, 7)):
                    bill_id = self.generate_qr_code("BILL")
                    await self.test_endpoint(
                        session, "POST", f"{BASE_URL}/bill/create",
                        data={
                            "bill_id": bill_id,
                            "description": f"Test bill {bill_id}",
                            "parent_bag_count": random.randint(1, 10)
                        }
                    )
                    await asyncio.sleep(random.uniform(1, 3))
                    
            elif behavior == 'admin':
                # Simulate admin operations
                admin_pages = ['/user_management', '/api/stats', '/dashboard', '/admin/system-integrity']
                for _ in range(random.randint(5, 10)):
                    page = random.choice(admin_pages)
                    await self.test_endpoint(session, "GET", f"{BASE_URL}{page}")
                    await asyncio.sleep(random.uniform(1, 2))
            
            logger.info(f"User {user_id} ({behavior}) completed workflow")
            
        except Exception as e:
            logger.error(f"User {user_id} workflow error: {str(e)}")
        finally:
            await session.close()
    
    async def run_concurrent_users(self, num_users):
        """Run multiple concurrent user simulations"""
        logger.info(f"Starting {num_users} concurrent user simulations...")
        
        tasks = []
        for i in range(num_users):
            task = asyncio.create_task(self.simulate_user_workflow(i))
            tasks.append(task)
            # Stagger user starts slightly
            await asyncio.sleep(0.1)
        
        # Wait for all users to complete
        await asyncio.gather(*tasks, return_exceptions=True)
        logger.info(f"All {num_users} users completed")
    
    async def continuous_load_test(self, duration_seconds):
        """Run continuous load test for specified duration"""
        logger.info(f"Running continuous load test for {duration_seconds} seconds...")
        
        end_time = time.time() + duration_seconds
        iteration = 0
        
        while time.time() < end_time:
            iteration += 1
            remaining = end_time - time.time()
            
            if remaining <= 0:
                break
                
            logger.info(f"Iteration {iteration} - Remaining: {remaining:.1f}s")
            
            # Run batch of concurrent users
            batch_size = min(CONCURRENT_USERS, int(remaining))
            if batch_size > 0:
                await self.run_concurrent_users(batch_size)
            
            # Brief pause between batches
            await asyncio.sleep(1)
    
    def monitor_system_resources(self):
        """Monitor and log system resources"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # Network statistics
        net_io = psutil.net_io_counters()
        
        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_used_gb": memory.used / (1024**3),
            "memory_total_gb": memory.total / (1024**3),
            "disk_percent": disk.percent,
            "network_sent_mb": net_io.bytes_sent / (1024**2),
            "network_recv_mb": net_io.bytes_recv / (1024**2)
        }
    
    async def test_all_endpoints(self):
        """Test all major endpoints once"""
        logger.info("Testing all major endpoints...")
        
        session = self.create_test_session()
        
        try:
            # Login first
            await self.login_user(session)
            
            # Test GET endpoints
            get_endpoints = [
                '/health',
                '/dashboard',
                '/bags',
                '/bills',
                '/scans',
                '/user_management',
                '/api/stats',
                '/api/scans',
                '/profile'
            ]
            
            for endpoint in get_endpoints:
                status, _ = await self.test_endpoint(session, "GET", f"{BASE_URL}{endpoint}")
                logger.info(f"  {endpoint}: {status}")
            
            # Test POST endpoints with sample data
            parent_qr = self.generate_qr_code("TEST_PARENT")
            child_qrs = [self.generate_qr_code("TEST_CHILD") for _ in range(5)]
            
            # Test scanning
            await self.test_endpoint(
                session, "POST", f"{BASE_URL}/scan/parent",
                json_data={"qr_code": parent_qr}
            )
            
            await self.test_endpoint(
                session, "POST", f"{BASE_URL}/process_child_scan_fast",
                json_data={"parent_qr": parent_qr, "child_qrs": child_qrs}
            )
            
            logger.info("All endpoints tested")
            
        finally:
            await session.close()

async def main():
    """Main test execution"""
    logger.info("=" * 80)
    logger.info("SYSTEM LOAD TEST FOR TRACETRACK")
    logger.info("=" * 80)
    logger.info(f"Configuration:")
    logger.info(f"  - Concurrent Users: {CONCURRENT_USERS}")
    logger.info(f"  - Test Duration: {TEST_DURATION_SECONDS} seconds")
    logger.info(f"  - Base URL: {BASE_URL}")
    logger.info("=" * 80)
    
    tester = LoadTester()
    
    # Record initial system state
    initial_resources = tester.monitor_system_resources()
    logger.info(f"Initial System State:")
    logger.info(f"  CPU: {initial_resources['cpu_percent']}%")
    logger.info(f"  Memory: {initial_resources['memory_percent']}%")
    logger.info(f"  Disk: {initial_resources['disk_percent']}%")
    
    try:
        # Test all endpoints first
        await tester.test_all_endpoints()
        
        # Run the main load test
        await tester.continuous_load_test(TEST_DURATION_SECONDS)
        
        # Get final metrics
        final_resources = tester.monitor_system_resources()
        summary = tester.metrics.get_summary()
        
        # Print results
        logger.info("\n" + "=" * 80)
        logger.info("LOAD TEST RESULTS")
        logger.info("=" * 80)
        
        logger.info(f"\nðŸ“Š PERFORMANCE METRICS:")
        logger.info(f"  Total Requests: {summary['total_requests']:,}")
        logger.info(f"  Successful: {summary['successful_requests']:,}")
        logger.info(f"  Failed: {summary['failed_requests']:,}")
        logger.info(f"  Error Rate: {summary['error_rate']:.2f}%")
        logger.info(f"  Requests/Second: {summary['requests_per_second']:.2f}")
        
        logger.info(f"\nâ±ï¸  RESPONSE TIMES:")
        logger.info(f"  Average: {summary['avg_response_time']:.3f}s")
        logger.info(f"  Min: {summary['min_response_time']:.3f}s")
        logger.info(f"  Max: {summary['max_response_time']:.3f}s")
        logger.info(f"  P50: {summary['p50_response_time']:.3f}s")
        logger.info(f"  P95: {summary['p95_response_time']:.3f}s")
        logger.info(f"  P99: {summary['p99_response_time']:.3f}s")
        
        logger.info(f"\nðŸ’¾ SYSTEM RESOURCES:")
        logger.info(f"  CPU Usage: {initial_resources['cpu_percent']}% â†’ {final_resources['cpu_percent']}%")
        logger.info(f"  Memory Usage: {initial_resources['memory_percent']:.1f}% â†’ {final_resources['memory_percent']:.1f}%")
        logger.info(f"  Network Sent: {(final_resources['network_sent_mb'] - initial_resources['network_sent_mb']):.2f} MB")
        logger.info(f"  Network Recv: {(final_resources['network_recv_mb'] - initial_resources['network_recv_mb']):.2f} MB")
        
        logger.info(f"\nðŸ“ˆ ENDPOINT PERFORMANCE:")
        for endpoint, stats in summary['endpoint_stats'].items():
            if stats['count'] > 0:
                avg_time = stats['total_time'] / stats['count']
                error_rate = (stats['errors'] / stats['count']) * 100
                logger.info(f"  {endpoint}:")
                logger.info(f"    Requests: {stats['count']}, Errors: {stats['errors']} ({error_rate:.1f}%)")
                logger.info(f"    Avg Time: {avg_time:.3f}s")
                logger.info(f"    Status Codes: {stats['status_codes']}")
        
        # Performance verdict
        logger.info("\n" + "=" * 80)
        logger.info("PERFORMANCE VERDICT")
        logger.info("=" * 80)
        
        # Analyze performance
        if summary['error_rate'] < 1 and summary['p95_response_time'] < 2:
            verdict = "âœ… EXCELLENT: System handles 50+ concurrent users with exceptional performance"
            ready_for_scale = True
        elif summary['error_rate'] < 5 and summary['p95_response_time'] < 5:
            verdict = "âœ… GOOD: System handles load well, ready for 50+ concurrent users"
            ready_for_scale = True
        elif summary['error_rate'] < 10 and summary['p95_response_time'] < 10:
            verdict = "âš ï¸  FAIR: System shows some strain, optimization recommended before scaling"
            ready_for_scale = False
        else:
            verdict = "âŒ POOR: System struggles with current load, immediate optimization required"
            ready_for_scale = False
        
        logger.info(verdict)
        
        if ready_for_scale:
            logger.info("\nâœ… System is ready to handle 50+ concurrent users and 800,000+ bags")
            logger.info("   Database pool configuration supports high concurrency")
            logger.info("   Response times are within acceptable limits")
            logger.info("   Error rate is minimal")
        else:
            logger.info("\nâš ï¸  System needs optimization before handling full load")
            logger.info("   Consider:")
            logger.info("   - Increasing database connection pool size")
            logger.info("   - Optimizing slow queries")
            logger.info("   - Adding caching layers")
            logger.info("   - Scaling worker processes")
        
        # Save detailed report
        report = {
            "timestamp": datetime.now().isoformat(),
            "configuration": {
                "concurrent_users": CONCURRENT_USERS,
                "test_duration": TEST_DURATION_SECONDS
            },
            "metrics": summary,
            "resources": {
                "initial": initial_resources,
                "final": final_resources
            },
            "verdict": verdict
        }
        
        report_file = f"load_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"\nðŸ“„ Detailed report saved to: {report_file}")
        
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())